# Ollama LLaMA2 Chatbot ğŸ¤–

A local AI chatbot built using **Streamlit**, **FastAPI**, and **Ollama (LLaMA2)**.  
This project demonstrates how to build a full-stack chatbot that runs completely on a local machine without relying on paid APIs.

---

## ğŸš€ Features

- ğŸ’¬ Interactive chat UI using Streamlit  
- âš¡ FastAPI backend for handling requests  
- ğŸ§  LLaMA2 model served locally via Ollama  
- ğŸ”„ Chat history maintained during session  
- ğŸ” Fully local & private (no external API calls)

---

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**
- **Streamlit** â€“ Frontend UI
- **FastAPI** â€“ Backend API
- **Ollama** â€“ Local LLaMA2 model
- **Requests** â€“ API communication

---

## ğŸ“‚ Project Structure

```text
ollama-llama2-chatbot/
â”‚
â”œâ”€â”€ app.py        # FastAPI backend
â”œâ”€â”€ chat.py       # Ollama / LLaMA2 interaction logic
â”œâ”€â”€ ui.py         # Streamlit frontend
â”œâ”€â”€ screenshots/  # Project screenshots
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md



# Ollama LLaMA2 Chatbot

A chatbot built using Streamlit, FastAPI, and Ollama LLaMA2.

## Screenshots

![Chat UI](screenshots/chat-ui.png)




#HOW TO US EMY PROJECT ON UR MACHINE
#FOLLOW THE STEPS BELOW


âš™ï¸ Setup & Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/sainiyogra5/ollama-llama2-chatbot.git
cd ollama-llama2-chatbot

2ï¸âƒ£ Create virtual environment
python -m venv .venv
.venv\Scripts\activate   # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

â–¶ï¸ How to Run
Start Ollama

Make sure Ollama is installed and the LLaMA2 model is pulled:

ollama pull llama2

Start Backend (FastAPI)
python app.py

Start Frontend (Streamlit)
streamlit run ui.py


Open browser at:

http://localhost:8501
